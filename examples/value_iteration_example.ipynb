{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example usage\n",
    "\n",
    "## Import the package with pip install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Install required packages\n",
    "! pip install git+https://github.com/CassandraDurr/value_iteration.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import functions from the package \n",
    "In this example we are going to perform value iteration in two ways. In the first example we will provide python inputs to the `value_iteration` function, and in the second example we will input the same data, but from `csv` format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from value_iteration import value_iteration, load_mdp_from_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Create Markov Decision Process (MDP) inputs\n",
    "\n",
    "# Define state space\n",
    "S = [\"healthy\", \"sick\"]\n",
    "\n",
    "# Define which actions (\"relax\" and \"party\") can occur at each state (\"healthy\" and \"sick\")\n",
    "A = {\n",
    "    \"healthy\": [\"relax\", \"party\"],\n",
    "    \"sick\": [\"relax\", \"party\"],\n",
    "}\n",
    "\n",
    "# Define the transition probabilities. The key is in the form (current state, action, next state).\n",
    "P = {\n",
    "    (\"healthy\", \"relax\", \"healthy\"): 0.95,\n",
    "    (\"healthy\", \"relax\", \"sick\"): 0.05,\n",
    "    (\"sick\", \"relax\", \"healthy\"): 0.5,\n",
    "    (\"sick\", \"relax\", \"sick\"): 0.5,\n",
    "    (\"healthy\", \"party\", \"healthy\"): 0.7,\n",
    "    (\"healthy\", \"party\", \"sick\"): 0.3,\n",
    "    (\"sick\", \"party\", \"healthy\"): 0.1,\n",
    "    (\"sick\", \"party\", \"sick\"): 0.9,\n",
    "}\n",
    "\n",
    "# Define the reward function. The key is in the form (current state, action).\n",
    "R = {\n",
    "    (\"healthy\", \"relax\"): 7,\n",
    "    (\"healthy\", \"party\"): 10,\n",
    "    (\"sick\", \"relax\"): 0,\n",
    "    (\"sick\", \"party\"): 2,\n",
    "}\n",
    "\n",
    "# Run value iteration algorithm\n",
    "optimal_values, optimal_policy = value_iteration(\n",
    "    S, A, P, R, gamma=0.9, theta=1e-9, printing=True\n",
    ")\n",
    "\n",
    "# Display results\n",
    "print(\"Optimal State Values:\", optimal_values)\n",
    "print(\"Optimal Policy:\", optimal_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Loading data from csv files\n",
    "\n",
    "# Obtain states, actions, transition probabilities and rewards\n",
    "S, A, P, R = load_mdp_from_csv(transitions_filepath=\"example_data/transitions.csv\")\n",
    "\n",
    "# Run value iteration\n",
    "optimal_values, optimal_policy = value_iteration(\n",
    "    S, A, P, R, gamma=0.9, theta=1e-9, printing=True,\n",
    ")\n",
    "\n",
    "# Display results\n",
    "print(\"Optimal State Values:\", optimal_values)\n",
    "print(\"Optimal Policy:\", optimal_policy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "value_itr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
