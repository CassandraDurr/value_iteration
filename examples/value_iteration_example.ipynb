{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example usage\n",
    "\n",
    "## Import the package with pip install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Install required packages\n",
    "! pip install git+https://github.com/CassandraDurr/value_iteration.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import functions from the package \n",
    "In this example we are going to perform value iteration in two ways. In the first example we will provide python inputs to the `value_iteration` function, and in the second example we will input the same data, but from `csv` format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from value_iteration import value_iteration, load_mdp_from_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, max value change: 10.0\n",
      "Iteration 2, max value change: 7.199999999999999\n",
      "Iteration 3, max value change: 0\n",
      "Optimal State Values: {'A': 17.2, 'B': 4.6, 'C': 8.0, 'D': 0.0}\n",
      "Optimal Policy: {'A': '2', 'B': '3', 'C': '2', 'D': '1'}\n"
     ]
    }
   ],
   "source": [
    "# Example 1: Create Markov Decision Process (MDP) inputs\n",
    "\n",
    "# Define state space\n",
    "S = [\"A\", \"B\", \"C\", \"D\"]\n",
    "\n",
    "# Define which actions (\"1\", \"2\", and \"3\") can occur at each state (\"A\", \"B\", \"C\", \"D\")\n",
    "A = {\n",
    "    \"A\": [\"1\", \"2\"],\n",
    "    \"B\": [\"1\", \"3\"],\n",
    "    \"C\": [\"2\", \"3\"],\n",
    "    \"D\": [\"1\"],\n",
    "}\n",
    "\n",
    "# Define the transition probabilities. The key is in the form (current state, action, next state).\n",
    "P = {\n",
    "    (\"A\", \"1\", \"B\"): 0.8,\n",
    "    (\"A\", \"1\", \"C\"): 0.2,\n",
    "    (\"A\", \"2\", \"C\"): 1.0,\n",
    "    (\"B\", \"1\", \"D\"): 1.0,\n",
    "    (\"B\", \"3\", \"C\"): 0.5,\n",
    "    (\"B\", \"3\", \"D\"): 0.5,\n",
    "    (\"C\", \"2\", \"D\"): 1.0,\n",
    "}\n",
    "\n",
    "# Define the reward function. The key is in the form (current state, action, next state).\n",
    "R = {\n",
    "    (\"A\", \"1\", \"B\"): 5,\n",
    "    (\"A\", \"1\", \"C\"): 10,\n",
    "    (\"A\", \"2\", \"C\"): 10,\n",
    "    (\"B\", \"1\", \"D\"): -1,\n",
    "    (\"B\", \"3\", \"C\"): 2,\n",
    "    (\"B\", \"3\", \"D\"): 0,\n",
    "    (\"C\", \"2\", \"D\"): 8,\n",
    "}\n",
    "\n",
    "# Run value iteration algorithm\n",
    "optimal_values, optimal_policy = value_iteration(\n",
    "    S, A, P, R, gamma=0.9, theta=1e-9, printing=True\n",
    ")\n",
    "\n",
    "# Display results\n",
    "print(\"Optimal State Values:\", optimal_values)\n",
    "print(\"Optimal Policy:\", optimal_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, max value change: 10.0\n",
      "Iteration 2, max value change: 7.199999999999999\n",
      "Iteration 3, max value change: 0\n",
      "Optimal State Values: {'A': 17.2, 'B': 4.6, 'C': 8.0, 'D': 0.0}\n",
      "Optimal Policy: {'A': '2', 'B': '3', 'C': '2', 'D': '1'}\n"
     ]
    }
   ],
   "source": [
    "# Example 2: Loading data from csv files\n",
    "\n",
    "# Obtain states, actions, transition probabilities and rewards\n",
    "S, A, P, R = load_mdp_from_csv(\n",
    "    transitions_filepath=\"example_data/transitions.csv\",\n",
    "    state_actions_filepath=\"example_data/state_actions.csv\",\n",
    ")\n",
    "\n",
    "# Run value iteration\n",
    "optimal_values, optimal_policy = value_iteration(\n",
    "    S, A, P, R, gamma=0.9, theta=1e-9, printing=True\n",
    ")\n",
    "\n",
    "# Display results\n",
    "print(\"Optimal State Values:\", optimal_values)\n",
    "print(\"Optimal Policy:\", optimal_policy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "value_itr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
